# BRSR Data Scraper and Aggregator

# Definitions

| Acronym/term | Meaning |
| --- | --- |
| LLM | Large Language Model. This is the AI model used to analyze the PDF report |
| BRSR | Business Responsibility and Sustainability Report - the legally required reporting on company emissions and sustainability practices, as part of CSR |
| GPT-4 | Most well documented for LLM with lots of automations and solid community support and expertise. Refer https://www.notion.so/GPT-4-vs-Claude-556c32461c5240deaee03434aaf2a3cf?pvs=21 |
| Claude | A new player on the market, with higher input limit than GPT-4 but less error, automation and community support. Much cheaper Refer https://www.notion.so/GPT-4-vs-Claude-556c32461c5240deaee03434aaf2a3cf?pvs=21 |
| BI tool | Business Intelligence - tooling used to draw comprehensive analytics and conclusions from the data obtained. e.g. Looker Studio |
| VPS | Virtual Platform-as-a-Service. Computing resources available over the internet, which can be used anytime, anywhere and can be configured to be extremely powerful and fast. |
| CSV | Comma-seperated Values. Used to specify insertion of data into an Excel spreadsheet. |
| API | Application processing interface. Used to leverage the power of online LLMs into the program. |
| GCP/OCP | Google/Oracle Cloud Platform- the best and most readily available VPS providers. GCP integrates the use of Looker Studio. |
| Token | It is the medium though which an LLM sees input and output - a collection of instructions conveyed as written language. 100 words are roughly 75 tokens, as is the ratio defined by OpenAI. |

# Inputs

1. BRSR reports as generated by corporates - from BSE and online portals, in PDF format
2. Data fields which will need to be extracted, as specified in the parameters given in a partially furnished Excel sheet. In-document navigation is handled as such:
    1. If a field specifies a reference to another part in the given document, the automation must make sure to follow the redirect and capture the correct data point.
    2. A base unit - on which all the captured values for a particular data point from different documents must be normalized.
        1. e.g. for power consumption: all the values must be in MW, for water consumption, in litres, etc.
3. Categorization of data fields into major categories so that data is extracted accordingly.
    1. More useful for analytics, however it can help split the workload and lighten the load on the LLM.

# Process:

1. **Parser Script**
    1. This will take a PDF as input and generate the values required. 
2. **Alternative: JavaScript automation**
    1. Browser automation consisting of prompt injection with required parameters
    2. Splitting PDF into 3-4 arbitrary equally-sized pieces
    3. Informing the LLM that these pieces are part of the same document.
3. **GUI (optional)**
    1. Web UI

# 1. Parse PDF Report

- LLM parsing
    - GPT 3.5 Turbo with File Search Assistant - OpenAI tools

# 2. Feed Relevant Parameters

- Not a very intensive process.
    - Excel sheet already contains parameters.
    - Tokenized queries to LLM to generate values in DF to XLSX format.

# 3. Generation of XLSX

- Per Category and Total values

# 4. Deliverables

a) The Program using GPT API and its source code.

b) Ownership license, granting JN Consulting full ownership of the software. Non-Disclosure Agreement (NDA) protecting the ownership and development of the project’s code.

b) Final Excel sheet for the extracted information in the various categories as well as the consolidated data itself for whatever reports the program will analyze in its development, testing and deployment process.

d) Web/native GUI for ease of use.

# Additional information

## 1. Cloud VPS and storage

- Secure collaboration.
- Much easier deployment.
- Website format

# Shortcomings

1. The pricing and availability of the APIs affects everything. 
2. The software is powered by LLMs, which are inherently imperfect in their reasoning, not too differently from humans. This presents a host of problems:
    1. **Only one Claude/GPT instance is used: it is bound to present errors** since - even with manual trimming, there is a lot of fluff to the documents: **considerable graphical artifacts (illustrations, pictures, colours etc)** which will generate a lot of extra, inseparable data which adds load.
    2. Ingest cost is **per instance used.** An error checking instance will actually cost more since it’s taking the input of the original, AND the output that came with it, increasing our load. This is impractical, we’ll be paying close to 2.5x the ingest cost for one document ($5 for Claude, $12.5 for GPT-4)
        1. We have to therefore maximize the accuracy of the original and work with that. This requires manual splitting of the document and asking the LLM to update the data in iterations, which means upto a +30% cost to the estimates I stated above.
        2. ***As such: the risk of inaccuracy, while lower than it has ever been in AI development history, still exists*. The liability of that rests *solely* with the API provider.**
        3. An Internet connection is required at all times to use this software. No local LLM is powerful enough to even attempt matching the accuracy of the proposed solutions, much less one accessible on the hardware available to us for $50 a month - we’d need close to $500 a month for that.
